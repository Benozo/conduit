# multi_llm_swarm

## ğŸ§  What It Does

This example demonstrates the **Multi-LLM Agent Swarm** architecture where each agent uses its own specialized LLM provider and model. It showcases optimal model selection for different tasks while managing costs and performance across multiple AI providers.

## âš™ï¸ Requirements

**Local Models (Optional):**
- **Ollama** - For fast, local agents
- **Models** - `llama3.2`, `qwen2.5` pulled locally

**Cloud APIs (Optional):**
- **OpenAI API Key** - For premium reasoning tasks
- **DeepInfra API Key** - For specialized code generation
- **Go 1.21+** - For running the swarm

## ğŸš€ How to Run

```bash
# 1. Setup local models (optional)
ollama serve
ollama pull llama3.2    # For coordinator 
ollama pull qwen2.5     # For content creation

# 2. Set API keys (optional)
export OPENAI_API_KEY="sk-your-openai-key"
export DEEPINFRA_API_KEY="your-deepinfra-key"

# 3. Run the multi-LLM swarm demo
go run main.go
```

## ğŸ” Agent Architecture

| Agent | Provider | Model | Purpose | Cost |
|-------|----------|-------|---------|------|
| **Coordinator** | Ollama | `llama3.2` | Fast task routing | Free |
| **ContentCreator** | Ollama | `qwen2.5` | Content generation | Free |
| **DataAnalyst** | OpenAI | `gpt-4` | Complex reasoning | Premium |
| **CodeGenerator** | DeepInfra | `Qwen Coder` | Code specialization | Mid-tier |

## ğŸ’¡ Sample Output

```bash
ğŸ¤– Multi-LLM Agent Swarm Demo
============================

ğŸ”§ Creating Specialized Agents:
âœ… Coordinator (Ollama llama3.2) - Task routing
âœ… ContentCreator (Ollama qwen2.5) - Text processing  
âœ… DataAnalyst (OpenAI gpt-4) - Complex analysis
âœ… CodeGenerator (DeepInfra Qwen Coder) - Code generation

ğŸ¯ Task: "Analyze user data and generate Python code for visualization"

ğŸ“ Coordinator: Routing to DataAnalyst for analysis...
ğŸ§  DataAnalyst (GPT-4): Analyzing patterns in user engagement data
ğŸ“ Coordinator: Routing to CodeGenerator for implementation...  
ğŸ’» CodeGenerator (Qwen Coder): Generating optimized Python visualization code

âœ… Result: Complete analysis + production-ready Python code
```

## ğŸ§ª Demo Scenarios

### 1. Cost-Optimized Processing
```
Simple text formatting â†’ Ollama (free)
Complex data analysis â†’ OpenAI (premium)
Code generation â†’ DeepInfra (specialized)
```

### 2. Performance-Tuned Pipeline
```
Fast routing â†’ Local llama3.2
Content creation â†’ Local qwen2.5  
Critical reasoning â†’ Cloud GPT-4
```

### 3. Provider Redundancy
```
Primary: Ollama models (always available)
Fallback: Cloud APIs (when local unavailable)
Specialized: Task-specific cloud models
```
go run main.go

# Full demo with real models (requires setup above)
go run main.go
```

## ğŸ—ï¸ Multi-LLM Architecture

### Agent Creation with Individual Models

```go
// Coordinator with fast local model for routing
coordinator := swarmClient.CreateAgentWithModel("coordinator",
    "Route tasks to appropriate agents", []string{},
    &swarm.ModelConfig{
        Provider:    "ollama",
        Model:       "llama3.2", 
        URL:         "http://localhost:11434",
        Temperature: 0.7,
    })

// Analyst with premium model for complex reasoning
analyst := swarmClient.CreateAgentWithModel("data_analyst", 
    "Perform complex analysis", []string{"analyze", "report"},
    &swarm.ModelConfig{
        Provider:    "openai",
        Model:       "gpt-4",
        APIKey:      os.Getenv("OPENAI_API_KEY"),
        Temperature: 0.3,
    })

// Code specialist with domain-specific model
coder := swarmClient.CreateAgentWithModel("code_generator",
    "Generate and review code", []string{"format", "validate"}, 
    &swarm.ModelConfig{
        Provider:    "deepinfra", 
        Model:       "Qwen/Qwen2.5-Coder-32B-Instruct",
        APIKey:      os.Getenv("DEEPINFRA_API_KEY"),
        Temperature: 0.1,
    })
```

### Backward Compatibility

```go
// Existing code continues to work unchanged
swarm := conduit.NewSwarmClient(mcpServer)
agent := swarm.CreateAgent("coordinator", "Route tasks", []string{})
// Uses swarm-level LLM (current behavior)
```

## ğŸ“Š Model Selection Strategy

| Agent Type | Provider | Model | Why? |
|------------|----------|-------|------|
| **Coordinator** | Ollama | llama3.2 | Fast routing decisions, always available |
| **ContentCreator** | Ollama | qwen2.5 | Better multilingual and content generation |
| **DataAnalyst** | OpenAI | GPT-4 | Premium reasoning for complex analysis |
| **CodeGenerator** | DeepInfra | Qwen Coder | Specialized for code generation tasks |

## ğŸ¯ Usage Patterns

### 1. Cost-Optimized Setup
- **Simple tasks**: Local Ollama models (free)
- **Complex tasks**: Cloud models (pay per use)
- **Routing**: Always local for speed and reliability

### 2. Performance-Optimized Setup  
- **Speed critical**: Small fast models (llama3.2:1b)
- **Quality critical**: Large models (GPT-4, Claude)
- **Specialized**: Domain-specific models (Codellama, Qwen Coder)

### 3. Reliability Setup
- **Primary**: Preferred models for each agent type
- **Fallback**: Swarm-level model as backup
- **Redundancy**: Multiple providers for critical agents

## ğŸ”§ Configuration Options

```go
type ModelConfig struct {
    Provider     string  // "ollama", "openai", "deepinfra"
    Model        string  // "llama3.2", "gpt-4", "qwen2.5" 
    URL          string  // Custom endpoints
    APIKey       string  // API authentication
    Temperature  float64 // Creativity level (0.0-1.0)
    MaxTokens    int     // Response length limit
    TopK         int     // Sampling diversity
}
```

## ğŸ“ˆ Expected Output

```
ğŸš€ Multi-LLM Agent Swarm Demo
=============================

ğŸ¤– Creating agents with different LLM providers:
   ğŸ“‹ coordinator - Ollama llama3.2 (fast routing)
   âœï¸  content_creator - Ollama qwen2.5 (optimized for content)
   ğŸ“Š data_analyst - OpenAI GPT-4 (premium reasoning)
   ğŸ’» code_generator - DeepInfra Qwen Coder (code specialist)

ğŸ¯ Multi-LLM Demo Scenarios:

ğŸ“ Scenario 1: Text Processing
ğŸ“„ Coordinator (llama3.2) routes to ContentCreator (qwen2.5)
ğŸ’¬ Request: Convert 'Hello Multi-LLM World' to snake_case format
ğŸ”„ Processing with ollama llama3.2...
âœ… Would route to appropriate specialist agent
...
```

## ğŸš€ Integration in Your Applications

### Basic Multi-LLM Setup

```go
// Create swarm with no default LLM
swarmClient := swarm.NewSwarmClient(server, nil)

// Create agents with specific models
routerAgent := swarmClient.CreateAgentWithModel("router", 
    instructions, tools, &swarm.ModelConfig{
    Provider: "ollama", Model: "llama3.2", // Fast & free
})

reasoningAgent := swarmClient.CreateAgentWithModel("reasoner",
    instructions, tools, &swarm.ModelConfig{
    Provider: "openai", Model: "gpt-4", // Powerful & accurate
})

// Use normally - agents automatically use their configured models
response := swarmClient.Run(routerAgent, messages, contextVars)
```

## ğŸ”— Related Examples

- **agent_swarm_llm**: Single-LLM swarm with Ollama
- **agent_swarm**: Rule-based multi-agent coordination
- **ollama**: Basic Ollama LLM integration
- **model_integration**: Custom model integration patterns

---

**ğŸ‰ Result**: A flexible, cost-efficient, and high-performance multi-agent system that uses the right model for each task!
