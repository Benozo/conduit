# agents_ollama

## ğŸ§  What It Does

An MCP-compatible AI agent that uses [Ollama](https://ollama.com) to respond to user input with intelligent tool usage. The agent can perform math calculations, search operations, memory management, and text processing using natural language understanding.

## âš™ï¸ Requirements

- **Ollama installed** - Download from [ollama.com](https://ollama.com)
- **AI model pulled** - `ollama pull llama3.2` (recommended)
- **Go 1.21+** - For building and running
- **4GB+ RAM** - For running the AI model

## ğŸš€ How to Run

```bash
# 1. Install and start Ollama
ollama serve

# 2. Pull a compatible model
ollama pull llama3.2

# 3. Set environment variables (optional)
export OLLAMA_HOST=http://localhost:11434    # Default
export OLLAMA_MODEL=llama3.2                 # Default

# 4. Run the agent
go run main.go
```

## ğŸ§ª Sample Prompts

Try these natural language requests with the agent:

- `"What is 23 * 12?"`
- `"Search for the capital of France"`
- `"Remember that my name is Alice"`
- `"Convert 'Hello World' to uppercase"`
- `"What's the square root of 144?"`

## âœ… Sample Output

```bash
ğŸ¤– Starting Ollama-powered AI Agent...
ğŸ”Œ Connected to Ollama at http://localhost:11434
ğŸ§  Using model: llama3.2

> What is 23 * 12?

ğŸ§  Agent Analysis: User wants to calculate 23 * 12
ğŸ”§ Tool Selection: math_multiply
âš¡ Executing: math_multiply(a=23, b=12)
âœ… Result: 276

Agent: I calculated 23 Ã— 12 for you. The answer is 276.

> Search for the capital of France

ğŸ§  Agent Analysis: User wants to search for geographical information
ğŸ”§ Tool Selection: search
âš¡ Executing: search(query="capital of France")
âœ… Result: Paris is the capital and largest city of France

Agent: I found that Paris is the capital of France. It's also the largest city in the country.

> Remember that my name is Alice

ğŸ§  Agent Analysis: User wants to store personal information
ğŸ”§ Tool Selection: remember
âš¡ Executing: remember(key="user_name", value="Alice")
âœ… Result: Information stored successfully

Agent: Got it! I'll remember that your name is Alice.
```

## ğŸ”§ Tools Available

The agent has access to these MCP tools:

- **Math Tools**: `add`, `multiply`, `divide`, `subtract`
- **Search Tools**: `search`, `web_lookup` 
- **Memory Tools**: `remember`, `recall`, `forget`, `list_memories`
- **Text Tools**: `uppercase`, `lowercase`, `reverse`, `word_count`
- **Utility Tools**: `timestamp`, `uuid`, `hash_md5`

## âš™ï¸ Configuration Options

| Environment Variable | Default | Description |
|---------------------|---------|-------------|
| `OLLAMA_HOST` | `http://localhost:11434` | Ollama server URL |
| `OLLAMA_MODEL` | `llama3.2` | Model to use for reasoning |
| `AGENT_PORT` | `8080` | HTTP server port (if applicable) |

## ğŸ¯ Key Features

- âœ… **Natural Language Processing**: Understands conversational requests
- âœ… **Intelligent Tool Selection**: Automatically chooses the right tools
- âœ… **Context Awareness**: Maintains conversation context and memory
- âœ… **Local AI**: Privacy-focused local model execution
- âœ… **Real-time Responses**: Fast response times with local models
- âœ… **Error Handling**: Graceful handling of tool failures and retries

## ğŸ” How It Works

1. **User Input** â†’ Natural language request
2. **AI Analysis** â†’ Ollama model analyzes intent and available tools  
3. **Tool Selection** â†’ Agent chooses appropriate MCP tools
4. **Execution** â†’ Tools execute with extracted parameters
5. **Response** â†’ Natural language response with results

## âš ï¸ Troubleshooting

**Ollama Connection Issues:**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama if not running
ollama serve
```

**Model Not Found:**
```bash
# List available models
ollama list

# Pull required model
ollama pull llama3.2
```

**Memory Issues:**
- Ensure at least 4GB RAM available
- Try smaller models: `ollama pull llama3.2:1b`
- Close other applications if needed

## ğŸ“š Related Examples

- [`ollama/`](../ollama) - Basic Ollama integration without agents
- [`agents_deepinfra/`](../agents_deepinfra) - Cloud-based agent alternative
- [`agent_swarm_simple/`](../agent_swarm_simple) - Multi-agent coordination
- [`pure_library/`](../pure_library) - Library-only usage without agents

## ğŸš€ Next Steps

After trying this example:

1. Experiment with different models: `qwen2.5`, `codellama`, `mistral`
2. Try the multi-agent swarm: [`agent_swarm_llm/`](../agent_swarm_llm)
3. Add custom tools: [`custom_tools/`](../custom_tools)
4. Deploy in production: [`openai/`](../openai) for cloud reliability
